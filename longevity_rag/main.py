import os 
from crawler.arxiv_scraper import fetch_papers, check_peer_validity
from rag.document_store import create_vector_store
from rag.qa_pipeline import generate_context
from llm.gpt_wrapper import ask_with_relevant_context

def main():
    print("ğŸ“š Fetching papers...")
    papers = fetch_papers("longevity biomarkers", max_results=10)

    print(f"ğŸ” Total fetched: {len(papers)}")

    valid_papers = []
    for i, paper in enumerate(papers, 1):
        print(f"\nğŸ” Checking paper {i}: {paper['title'][:80]}...")
        validity = check_peer_validity(paper, verbose=True)  # Enable verbose logging

        if validity["valid"]:
            print(f"âœ… VALID â€” Venue: {validity['venue']}, Citations: {validity['citationCount']}")
            paper.update(validity)  # Optional: attach metadata
            valid_papers.append(paper)
        else:
            print(f"âŒ INVALID â€” Reason: {validity['reason']}")

    print(f"\nğŸ“Š Valid papers found: {len(valid_papers)}")

    if not valid_papers:
        print("ğŸš« No valid papers found. Exiting.")
        return

    print("ğŸ§  Creating vector store...")
    vectorstore = create_vector_store(valid_papers)

    question = "What are the factors that increase lifespan?"
    print(f"\nâ“ Question: {question}")
    
    print("ğŸ¤– Calling Chat Gpt 3.5 model...")
    answer = ask_with_relevant_context(question, vectorstore)

    print(f"\nğŸ’¡ Answer: {answer}")


if __name__ == "__main__":
    main()
